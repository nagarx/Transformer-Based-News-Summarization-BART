{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Environment Setup and Core Libraries\n",
    "\n",
    "This cell initializes the essential environment and libraries for our text summarization project. We import:\n",
    "- **Standard Libraries**: `numpy` and `pandas` for data handling.\n",
    "- **PyTorch**: For neural network operations.\n",
    "- **Transformers Library**: From Hugging Face, providing access to BART model utilities.\n",
    "- **Evaluation and Tracking Tools**: `evaluate` for model performance evaluation and `wandb` for experiment tracking.\n",
    "- **Hugging Face Hub Utilities**: Optional, for model management.\n",
    "- **Version Information**: Of `transformers` library for compatibility and debugging.\n",
    "\n",
    "This setup creates a robust foundation for implementing and tracking the performance of our text summarization model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45f155015643021d"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:27:11.781264Z",
     "start_time": "2023-11-13T18:27:08.159108Z"
    }
   },
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch for neural networks\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import cuda\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Hugging Face transformers for NLP tasks\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Evaluation and experiment tracking tools\n",
    "import evaluate  # For evaluating model performance\n",
    "import wandb  # For experiment tracking and logging\n",
    "\n",
    "# Optional: Hugging Face Hub utilities\n",
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "# Version information (can be used for logging or debugging)\n",
    "from transformers import __version__ as transformers_version"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initializing Weights & Biases for Experiment Tracking\n",
    "\n",
    "This cell is dedicated to logging into Weights & Biases (wandb), a tool used for tracking and visualizing the progress and results of machine learning experiments. Key points:\n",
    "\n",
    "- **wandb Integration**: By executing `wandb.login()`, we establish a connection with the wandb service. This integration allows us to monitor various metrics during model training and evaluation, such as loss, accuracy, and more.\n",
    "- **Security Note**: It is crucial to set the wandb API key in the environment variables for secure access. This approach ensures that our credentials are not exposed in the notebook.\n",
    "\n",
    "Setting up wandb is a critical step for maintaining a systematic and thorough record of our model's performance throughout the training and evaluation phases."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8acf016225b264de"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33ma-elnagar3003\u001B[0m (\u001B[33mnigoo\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Login to Weights & Biases for experiment tracking\n",
    "# Note: Ensure that your wandb API key is set in your environment variables for security purposes\n",
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:27:13.137515Z",
     "start_time": "2023-11-13T18:27:11.804188Z"
    }
   },
   "id": "7c4015f682791623"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hugging Face Hub Interpreter Login\n",
    "\n",
    "In this cell, we establish a connection with the Hugging Face Hub using the `interpreter_login()` function. The Hugging Face Hub is a platform for sharing and discovering machine learning models, datasets, and evaluation metrics. Important aspects:\n",
    "\n",
    "- **Hugging Face Hub Connection**: The `interpreter_login()` function is used to log into the Hugging Face Hub. This connection enables us to interact with the platform, providing access to a vast repository of models and datasets.\n",
    "- **Security Measures**: It's essential to set the Hugging Face API keys (both for reading and writing) in the environment variables. This practice ensures the security of our credentials and seamless integration with the Hub.\n",
    "\n",
    "Logging into the Hugging Face Hub is a vital step for accessing state-of-the-art models, datasets, and tools that can enhance our text summarization project."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af00241021a425f7"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "    \n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/nigoo/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Login to Hugging Face Hub interpreter\n",
    "# Note: Set your Hugging Face API keys (for reading and writing) in your environment variables for security\n",
    "interpreter_login()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:27:20.540268Z",
     "start_time": "2023-11-13T18:27:13.137799Z"
    }
   },
   "id": "8dfe8361db833b72"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting Up Computation Device\n",
    "\n",
    "This cell is focused on configuring the computation device for our model training and inference tasks. It involves:\n",
    "\n",
    "- **CUDA Availability Check**: We use `cuda.is_available()` from PyTorch to check if a CUDA-enabled GPU is available. CUDA (Compute Unified Device Architecture) by NVIDIA is crucial for accelerating deep learning tasks.\n",
    "- **Device Setting**: Based on the availability, we set the `device` variable to either `'cuda'` (for GPU) or `'cpu'` (for CPU). This ensures that our model utilizes the most efficient available computing resource.\n",
    "- **Output Confirmation**: The cell outputs the type of device being used (`cuda` or `cpu`), providing clear feedback about the environment setup.\n",
    "\n",
    "Properly configuring the computation device is essential for efficient model training and can significantly impact the performance and speed of our text summarization task."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2eb2fd6269e886d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Determine if CUDA (GPU support) is available and set the device accordingly\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:27:20.540497Z",
     "start_time": "2023-11-13T18:27:20.538193Z"
    }
   },
   "id": "8e7c31789ede1a85"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Weights & Biases Project Initialization and Configuration Setup\n",
    "\n",
    "This cell plays a critical role in setting up the experiment tracking and defining the training configuration:\n",
    "\n",
    "- **Weights & Biases Project**: We initialize a new project in Weights & Biases (wandb) named \"BART_summarization\". This project will track all the metrics, logs, and outputs of our model training and evaluation, enabling us to monitor the experiment's progress and performance.\n",
    "\n",
    "- **Training Configuration Parameters**: The configuration settings for the training process are defined and stored in `wandb.config`. Key parameters include:\n",
    "  - `TRAIN_BATCH_SIZE` and `VALID_BATCH_SIZE`: Batch sizes for training and validation.\n",
    "  - `TRAIN_EPOCHS`: The number of epochs for training the model.\n",
    "  - `LEARNING_RATE`: Learning rate for the optimizer.\n",
    "  - `SEED`: A seed value for ensuring reproducibility of results.\n",
    "  - `MAX_LEN` and `SUMMARY_LEN`: Maximum length of input text and target length of the summary, respectively.\n",
    "\n",
    "- **Reproducibility**: To ensure consistent results across runs, we set seeds for PyTorch and NumPy, and make CUDA operations deterministic.\n",
    "\n",
    "- **Hugging Face Hub Repository Configuration**: We specify `new_repo` for creating a new repository and `repo_name` for the full name of the repository, including the namespace. This setup is essential for storing and managing the model on the Hugging Face Hub.\n",
    "\n",
    "These configurations are crucial for maintaining a structured and reproducible training process, as well as for effective tracking and management of the model training lifecycle."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "206eb0f40d31076a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.12"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/nigoo/Documents/Code/My_GitHub_Repositories/Text_Summarization_using_Transformers/notebooks/wandb/run-20231113_192720-z2z087rr</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/nigoo/BART_summarization/runs/z2z087rr' target=\"_blank\">rose-voice-25</a></strong> to <a href='https://wandb.ai/nigoo/BART_summarization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/nigoo/BART_summarization' target=\"_blank\">https://wandb.ai/nigoo/BART_summarization</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/nigoo/BART_summarization/runs/z2z087rr' target=\"_blank\">https://wandb.ai/nigoo/BART_summarization/runs/z2z087rr</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize a Weights & Biases project for experiment tracking\n",
    "wandb.init(project=\"BART_summarization\")\n",
    "\n",
    "# Setting configuration parameters for the training process\n",
    "config = wandb.config\n",
    "config.TRAIN_BATCH_SIZE = 2    # Training batch size\n",
    "config.VALID_BATCH_SIZE = 2    # Validation batch size\n",
    "config.TRAIN_EPOCHS = 2        # Number of training epochs\n",
    "config.LEARNING_RATE = 1e-4    # Learning rate\n",
    "config.SEED = 42               # Seed for reproducibility\n",
    "config.MAX_LEN = 512           # Maximum length of the input text\n",
    "config.SUMMARY_LEN = 150       # Target length of the summary\n",
    "\n",
    "# Set seeds for reproducibility across runs\n",
    "torch.manual_seed(config.SEED)\n",
    "np.random.seed(config.SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Repository configuration for Hugging Face Hub\n",
    "new_repo = \"text_summarizer\"                              # Name for a new repository\n",
    "repo_name = \"EducativeCS2023/bart-base-summarization\"     # Full name of the repository including namespace"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:27:24.933827Z",
     "start_time": "2023-11-13T18:27:20.545506Z"
    }
   },
   "id": "493e8f807870e405"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "In this cell, we focus on preparing the dataset for the text summarization task. The key steps include:\n",
    "\n",
    "- **Dataset Loading**: We load the dataset from a CSV file (`BBCarticles.csv`). The dataset contains articles and their corresponding summaries.\n",
    "\n",
    "- **Data Preparation**: \n",
    "  - We select the relevant columns, typically the text of the articles and their summaries.\n",
    "  - The text in the dataset is prepended with 'summarize: ' to signal our model that the task is summarization.\n",
    "\n",
    "- **Dataset Splitting**:\n",
    "  - We define a `split_ratio` to determine the proportion of data used for training and evaluation.\n",
    "  - The dataset is then split into two parts: one for training and another for evaluation, ensuring a balanced representation of data in both sets.\n",
    "\n",
    "- **Dataset Overview**:\n",
    "  - The shape (number of samples) of both training and evaluation datasets is printed to provide an overview of the dataset sizes.\n",
    "  - The first few rows of the dataframe are displayed to give a glimpse of the data format and content.\n",
    "\n",
    "This preprocessing step is crucial for setting up our dataset correctly for training the BART model and evaluating its performance on text summarization."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7c7f72251a2abac"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Size: (56, 2)\n",
      "Evaluation Dataset Size: (54, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                Text  \\\n0  summarize: Ad sales boost Time Warner profit\\n...   \n1  summarize: Dollar gains on Greenspan speech\\n\\...   \n2  summarize: Yukos unit buyer faces loan claim\\n...   \n\n                                             Summary  \n0  TimeWarner said fourth quarter sales rose 2% t...  \n1  The dollar has hit its highest level against t...  \n2  Yukos' owner Menatep Group says it will ask Ro...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>summarize: Ad sales boost Time Warner profit\\n...</td>\n      <td>TimeWarner said fourth quarter sales rose 2% t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>summarize: Dollar gains on Greenspan speech\\n\\...</td>\n      <td>The dollar has hit its highest level against t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>summarize: Yukos unit buyer faces loan claim\\n...</td>\n      <td>Yukos' owner Menatep Group says it will ask Ro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/BBCarticles.csv', encoding='latin-1')\n",
    "\n",
    "# Select relevant columns and prepend 'summarize: ' to the text\n",
    "df = df[['Text', 'Summary']]\n",
    "df.Text = 'summarize: ' + df.Text\n",
    "\n",
    "# Splitting the dataset into train and evaluation sets\n",
    "split_ratio = 0.025  # Split ratio for sampling\n",
    "# Creating a training dataset\n",
    "train_dataset = df.sample(frac=split_ratio, random_state=config.SEED).reset_index(drop=True)\n",
    "# Creating an evaluation dataset\n",
    "eval_dataset = df.drop(train_dataset.index).sample(frac=split_ratio, random_state=config.SEED).reset_index(drop=True)\n",
    "\n",
    "# Displaying the shape of the datasets\n",
    "print(\"Training Dataset Size:\", train_dataset.shape)\n",
    "print(\"Evaluation Dataset Size:\", eval_dataset.shape)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:27:25.061891Z",
     "start_time": "2023-11-13T18:27:24.934964Z"
    }
   },
   "id": "b7b13f241f31de11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom Dataset Class for Text Summarization\n",
    "\n",
    "This cell defines a `CustomDataset` class, which is crucial for structuring our data for the summarization task using the BART model. Key aspects of this class include:\n",
    "\n",
    "- **Initialization**:\n",
    "  - The constructor (`__init__`) takes a DataFrame, a tokenizer, and maximum lengths for the source and summary texts.\n",
    "  - The DataFrame contains the articles and their summaries, and the tokenizer is used for encoding the texts.\n",
    "\n",
    "- **Length Method** (`__len__`):\n",
    "  - This method returns the number of items in the dataset, which is essential for iterating over the dataset during training and evaluation.\n",
    "\n",
    "- **Get Item Method** (`__getitem__`):\n",
    "  - Retrieves a specific item from the dataset by its index.\n",
    "  - Each item consists of the article and its summary, which are preprocessed and encoded using the provided tokenizer.\n",
    "  - The method returns a dictionary containing the encoded source text, its attention mask, and the encoded target summary.\n",
    "\n",
    "This custom dataset class is fundamental for efficiently processing and feeding the data into our model for training and evaluation, ensuring that the data is in the correct format and adequately preprocessed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8340fd08ddba9688"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading articles and summaries into the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        :param dataframe: DataFrame containing the articles and summaries.\n",
    "        :param tokenizer: Tokenizer for encoding the texts.\n",
    "        :param source_len: Max length for the source text.\n",
    "        :param summ_len: Max length for the summary text.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = summ_len\n",
    "        self.Summary = self.data.Summary\n",
    "        self.Text = self.data.Text\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of items in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.Summary)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieves an item by its index.\n",
    "        :param index: Index of the desired item.\n",
    "        :return: Dictionary containing encoded source and target texts.\n",
    "        \"\"\"\n",
    "        # Adjusted preprocessing to match original notebook\n",
    "        Text = str(self.Text[index])\n",
    "        Text = ' '.join(Text.split())\n",
    "\n",
    "        Summary = str(self.Summary[index])\n",
    "        Summary = ' '.join(Summary.split())\n",
    "\n",
    "        # Encoding the source and target texts\n",
    "        source_encoded = self.tokenizer(Text, max_length=self.source_len, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        target_encoded = self.tokenizer(Summary, max_length=self.summ_len, padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "        # Extracting the encoded ids and attention masks\n",
    "        source_ids = source_encoded['input_ids'].squeeze()\n",
    "        source_mask = source_encoded['attention_mask'].squeeze()\n",
    "        target_ids = target_encoded['input_ids'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long)\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:27:25.066022Z",
     "start_time": "2023-11-13T18:27:25.060918Z"
    }
   },
   "id": "7c1d62af0773e42"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizer Initialization and DataLoader Setup\n",
    "\n",
    "This cell focuses on preparing the tokenizer and data loaders, essential components for processing and feeding data into our model:\n",
    "\n",
    "- **BART Tokenizer Initialization**:\n",
    "  - We initialize the BART tokenizer using `BartTokenizer.from_pretrained(repo_name)`. The tokenizer is crucial for converting text data into a format that can be processed by our BART model.\n",
    "  - The tokenizer configuration is then pushed to a new repository on the Hugging Face Hub using `tokenizer.push_to_hub(new_repo)`. This step is important for version control and sharing of the tokenizer configuration.\n",
    "\n",
    "- **Custom Dataset Instances**:\n",
    "  - We create instances of our `CustomDataset` class for both training (`training_set`) and evaluation (`eval_set`), passing the respective datasets, tokenizer, and configuration parameters like maximum text lengths.\n",
    "\n",
    "- **DataLoaders Initialization**:\n",
    "  - `DataLoader` objects for both training and evaluation datasets are created. The `DataLoader` batches data and provides an efficient iterator over the datasets.\n",
    "  - The training data loader is set to shuffle the data (`shuffle=True`), which is a good practice for training phases to introduce randomness and improve model generalization.\n",
    "  - The evaluation data loader does not require shuffling (`shuffle=False`), as the order of data does not impact the evaluation phase.\n",
    "\n",
    "These steps ensure that our data is correctly tokenized and batched, ready for the training and evaluation processes in our text summarization project."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c9a6fada3e72dd3"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Initialize the BART tokenizer from the pretrained model repository\n",
    "tokenizer = BartTokenizer.from_pretrained(repo_name)\n",
    "\n",
    "# Push the tokenizer configuration to the Hugging Face Hub\n",
    "tokenizer.push_to_hub(new_repo)\n",
    "\n",
    "# Create custom datasets for training and evaluation\n",
    "training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
    "eval_set = CustomDataset(eval_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
    "\n",
    "# Initialize DataLoaders for the training and evaluation sets\n",
    "# The DataLoader batches data and provides an iterator over the dataset\n",
    "training_loader = DataLoader(\n",
    "    training_set, \n",
    "    batch_size=config.TRAIN_BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=0  # Number of subprocesses for data loading\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    eval_set, \n",
    "    batch_size=config.VALID_BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=0  # Set to 0 since we don't need to shuffle evaluation data\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:27:27.070521Z",
     "start_time": "2023-11-13T18:27:25.064306Z"
    }
   },
   "id": "c4e9eb54304c0328"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BART Model Initialization and Optimization Setup\n",
    "\n",
    "This cell is critical for setting up our BART model and its optimization parameters:\n",
    "\n",
    "- **BART Model Initialization**:\n",
    "  - We initialize the BART model for conditional generation using `BartForConditionalGeneration.from_pretrained(repo_name)`. This pretrained model is well-suited for text summarization tasks.\n",
    "  - The model is then moved to the designated computation device (GPU or CPU) determined earlier using `model.to(device)`. This ensures that the model utilizes the most efficient computing resources available.\n",
    "\n",
    "- **Optimizer Setup**:\n",
    "  - An optimizer is crucial for the training process as it updates the model parameters based on the gradients. We choose the Adam optimizer, known for its efficiency in handling large datasets and complex architectures.\n",
    "  - The learning rate for the optimizer is set according to the predefined configuration (`config.LEARNING_RATE`). The learning rate is a key hyperparameter that influences the speed and quality of the training process.\n",
    "\n",
    "- **Weights & Biases Integration**:\n",
    "  - The model is integrated with Weights & Biases (wandb) using `wandb.watch(model, log=\"all\")`. This integration allows us to monitor various aspects of the model during training, including parameters, gradients, and more.\n",
    "  - This step is essential for keeping track of the model's performance and making informed adjustments during the training process.\n",
    "\n",
    "Setting up the BART model and its optimizer correctly, along with integrating it with Weights & Biases, lays a solid foundation for the effective training and monitoring of our text summarization model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "161ede570cb08394"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the BART model for conditional generation from the pretrained model repository\n",
    "model = BartForConditionalGeneration.from_pretrained(repo_name)\n",
    "\n",
    "# Move the model to the designated device (GPU or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up the optimizer for training the model\n",
    "# Adam optimizer is used with the specified learning rate from the configuration\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=config.LEARNING_RATE)\n",
    "\n",
    "# Integrate the model with Weights & Biases for monitoring and tracking during training\n",
    "# This allows for logging all model parameters and gradients\n",
    "wandb.watch(model, log=\"all\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:27:30.161506Z",
     "start_time": "2023-11-13T18:27:27.076672Z"
    }
   },
   "id": "fb7eed06ba5edd8b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Function Definition for Model Training\n",
    "\n",
    "This cell defines the `train` function, a crucial part of our training pipeline:\n",
    "\n",
    "- **Function Purpose**:\n",
    "  - The `train` function is designed to train the BART model for one epoch. It takes parameters like the current epoch number, tokenizer, model, device, data loader, and optimizer.\n",
    "\n",
    "- **Training Mode and Batch Processing**:\n",
    "  - The model is set to training mode using `model.train()`.\n",
    "  - The function iterates over batches of data from the `loader`. Each batch consists of input and target data that are prepared for the model.\n",
    "\n",
    "- **Loss Calculation and Logging**:\n",
    "  - The forward pass involves computing the model's output and calculating the loss.\n",
    "  - Special attention is paid to ignoring padding tokens in the loss calculation.\n",
    "  - The training loss is logged to Weights & Biases at regular intervals for monitoring.\n",
    "\n",
    "- **Weight Update Process**:\n",
    "  - The backward pass includes clearing previous gradients, computing new gradients, and updating the model weights.\n",
    "  - This process is crucial for the learning and improvement of the model over each batch of data.\n",
    "\n",
    "- **Feedback and Monitoring**:\n",
    "  - The function provides feedback on the training progress by printing the loss at regular intervals.\n",
    "  - This feedback is valuable for understanding the model's learning dynamics and making necessary adjustments.\n",
    "\n",
    "The `train` function is a key component in our training loop, ensuring systematic and efficient training of our model over each epoch."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc1ac6467ee29aa2"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    \"\"\"\n",
    "    Function to be called for training the model for one epoch.\n",
    "    :param epoch: Current epoch number.\n",
    "    :param tokenizer: Tokenizer for the text data.\n",
    "    :param model: The BART model for conditional generation.\n",
    "    :param device: The device (CPU or GPU) to use for training.\n",
    "    :param loader: DataLoader fofr the training data.\n",
    "    :param optimizer: Optimizer for updating model weights.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    for batch_index, data in enumerate(loader, 0):\n",
    "        # Prepare the input and target data\n",
    "        y = data['target_ids'].to(device, dtype=torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        labels = y[:, 1:].clone().detach()\n",
    "        labels[y[:, 1:] == tokenizer.pad_token_id] = -100  # Ignore padding tokens in loss calculation\n",
    "        ids = data['source_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype=torch.long)\n",
    "\n",
    "        # Forward pass: Compute the model output and calculate loss\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, decoder_input_ids=y_ids, labels=labels)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Log the training loss to Weights & Biases every 10 batches\n",
    "        if batch_index % 10 == 0:\n",
    "            wandb.log({\"Training Loss\": loss.item()})\n",
    "\n",
    "        # Print the loss every 500 batches\n",
    "        if batch_index % 500 == 0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        # Backward pass: Update model weights\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        loss.backward()        # Compute gradients\n",
    "        optimizer.step()       # Update weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:27:30.167362Z",
     "start_time": "2023-11-13T18:27:30.163331Z"
    }
   },
   "id": "c33c8e15ed2daca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Training Loop and Pushing to Hugging Face Hub\n",
    "\n",
    "This cell contains the core training loop and code to push the trained model to the Hugging Face Hub:\n",
    "\n",
    "- **Training Loop**:\n",
    "  - We iterate over a number of epochs as specified in the `config.TRAIN_EPOCHS`.\n",
    "  - For each epoch, we call the `train` function defined earlier, passing the necessary parameters like the current epoch, tokenizer, model, device, training data loader, and optimizer.\n",
    "  - This loop is where the model is effectively trained, learning to generate summaries from the input text.\n",
    "\n",
    "- **Progress Monitoring**:\n",
    "  - The progress of training is printed at the start of each epoch, providing visibility into the training process.\n",
    "\n",
    "- **Pushing Model to Hugging Face Hub**:\n",
    "  - After the training is complete, the model is pushed to a new repository on the Hugging Face Hub using `model.push_to_hub(new_repo)`.\n",
    "  - This step is significant for version control, sharing, and potentially deploying the model for future use.\n",
    "\n",
    "The execution of this cell marks the completion of the model's training phase and its availability for wider use through the Hugging Face Hub."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30a188d72f10f912"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1/2\n",
      "Epoch: 0, Loss:  1.5276191234588623\n",
      "Training epoch: 2/2\n",
      "Epoch: 1, Loss:  0.11024223268032074\n",
      "Model successfully pushed to Hugging Face Hub\n"
     ]
    }
   ],
   "source": [
    "# Training loop over the specified number of epochs\n",
    "for epoch in range(config.TRAIN_EPOCHS):\n",
    "    print(f\"Training epoch: {epoch+1}/{config.TRAIN_EPOCHS}\")\n",
    "    # Call the train function for each epoch\n",
    "    train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "# Push the trained model to the Hugging Face Hub\n",
    "model.push_to_hub(new_repo)\n",
    "print(\"Model successfully pushed to Hugging Face Hub\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:28:58.627177Z",
     "start_time": "2023-11-13T18:27:30.166940Z"
    }
   },
   "id": "db93f53dcdb9d466"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction Function Definition\n",
    "\n",
    "This cell introduces the `predict` function, an essential component for evaluating our text summarization model:\n",
    "\n",
    "- **Function Purpose**:\n",
    "  - The `predict` function is designed to generate predictions from the BART model. It takes parameters such as the tokenizer, model, device, and data loader for prediction.\n",
    "  \n",
    "- **Evaluation Mode and Prediction Generation**:\n",
    "  - The model is set to evaluation mode using `model.eval()`, which is necessary for making predictions as it disables certain layers like Dropout.\n",
    "  - Predictions are generated in a loop over the data from the `loader`, where input data is prepared and fed into the model.\n",
    "  \n",
    "- **Prediction and Decoding**:\n",
    "  - The `model.generate` method is used to generate predictions. Parameters like maximum length, beam search settings, and penalties for repetition and length are specified to control the generation process.\n",
    "  - Generated summaries are then decoded from their tokenized form to human-readable text using the tokenizer.\n",
    "\n",
    "- **Progress Monitoring**:\n",
    "  - The progress of prediction generation is printed at regular intervals to provide feedback on the completion of batches.\n",
    "\n",
    "- **Results Compilation**:\n",
    "  - The function returns two lists: `predictions` (containing the generated summaries) and `actuals` (containing the actual summaries from the dataset).\n",
    "\n",
    "This `predict` function is a vital tool for assessing the performance of our model by comparing its generated summaries against the actual summaries."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "619eee71d475a5e5"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def predict(tokenizer, model, device, loader):\n",
    "    \"\"\"\n",
    "    Function to generate predictions from the model.\n",
    "    :param tokenizer: Tokenizer for the text data.\n",
    "    :param model: The BART model for conditional generation.\n",
    "    :param device: The device (CPU or GPU) to use for prediction.\n",
    "    :param loader: DataLoader for the data to be predicted.\n",
    "    :return: Lists of predictions and actual summaries.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch_index, data in enumerate(loader, 0):\n",
    "            # Prepare the input data\n",
    "            ids = data['source_ids'].to(device, dtype=torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype=torch.long)\n",
    "            y = data['target_ids'].to(device, dtype=torch.long)\n",
    "\n",
    "            # Generate predictions\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=ids,\n",
    "                attention_mask=mask,\n",
    "                max_length=150,  # Maximum length of the generated summaries\n",
    "                num_beams=2,  # Number of beams for beam search\n",
    "                repetition_penalty=2.5,  # Penalty for repetition\n",
    "                length_penalty=1.0,  # Penalty for summary length\n",
    "                early_stopping=True  # Stop generating when all beams reach the EOS token\n",
    "            )\n",
    "            # Decode the generated summaries\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            # Decode the actual summaries\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in y]\n",
    "\n",
    "            # Print progress every 100 batches\n",
    "            if batch_index % 100 == 0:\n",
    "                print(f'Completed {batch_index} batches')\n",
    "\n",
    "            # Extend the lists with predictions and actuals\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "\n",
    "    return predictions, actuals"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:28:58.631396Z",
     "start_time": "2023-11-13T18:28:58.624754Z"
    }
   },
   "id": "7ac42c4fe202d2a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating Predictions and Compiling Results\n",
    "\n",
    "This cell executes the prediction process and compiles the results:\n",
    "\n",
    "- **Timing the Prediction Process**:\n",
    "  - The start time is recorded using `time.time()` to measure the duration of the prediction process.\n",
    "\n",
    "- **Prediction Generation**:\n",
    "  - The `predict` function is called with the necessary parameters (tokenizer, model, device, evaluation data loader) to generate predictions on the evaluation set.\n",
    "  - The function returns two lists: `predictions` (generated summaries) and `actuals` (actual summaries from the dataset).\n",
    "\n",
    "- **Results Compilation**:\n",
    "  - A DataFrame `results` is created to hold both the predictions and actual summaries, facilitating easy comparison and analysis.\n",
    "  - This DataFrame is then saved to a CSV file (`predictions.csv`), allowing for further analysis or sharing of the results.\n",
    "\n",
    "- **Time Evaluation**:\n",
    "  - The end time is recorded, and the time taken for the prediction process is calculated and printed. This information is useful for evaluating the efficiency of the prediction process.\n",
    "  \n",
    "- **Results Display**:\n",
    "  - The first few rows of the results DataFrame are displayed to provide a quick overview of the model's performance.\n",
    "\n",
    "This step is crucial in evaluating the effectiveness of our text summarization model by comparing its predictions against the actual summaries."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "764ba84bad023678"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0 batches\n",
      "Time taken for predictions: 147.54 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                         predictions  \\\n0   mobile phone users to send multimedia message...   \n1   go into the game on the back of a 2-0 victory...   \n2   a statement, Media Labs Europe said the decis...   \n3   Knapman rejected the idea Mr Kilroy-Silk pose...   \n4   proponents of the bill said it was a good com...   \n\n                                             actuals  \n0  Getting mobile phone users to send multimedia ...  \n1  Arsenal go into the game on the back of a 2-0 ...  \n2  In a statement, Media Labs Europe said the dec...  \n3  Mr Knapman rejected the idea Mr Kilroy-Silk po...  \n4  The European Parliament has thrown out a bill ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>predictions</th>\n      <th>actuals</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>mobile phone users to send multimedia message...</td>\n      <td>Getting mobile phone users to send multimedia ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>go into the game on the back of a 2-0 victory...</td>\n      <td>Arsenal go into the game on the back of a 2-0 ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>a statement, Media Labs Europe said the decis...</td>\n      <td>In a statement, Media Labs Europe said the dec...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Knapman rejected the idea Mr Kilroy-Silk pose...</td>\n      <td>Mr Knapman rejected the idea Mr Kilroy-Silk po...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>proponents of the bill said it was a good com...</td>\n      <td>The European Parliament has thrown out a bill ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure the start time for the prediction process\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate predictions using the defined prediction function\n",
    "predictions, actuals = predict(tokenizer, model, device, eval_loader)\n",
    "\n",
    "# Compile the predictions and actual summaries into a DataFrame\n",
    "results = pd.DataFrame({'predictions': predictions, 'actuals': actuals})\n",
    "\n",
    "# Save the results to a CSV file for further analysis or sharing\n",
    "results.to_csv('../results/predictions.csv')\n",
    "\n",
    "# Measure the end time and calculate the time taken for prediction\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "print(f\"Time taken for predictions: {time_taken:.2f} seconds\")\n",
    "\n",
    "# Display the first few rows of the results\n",
    "results.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:31:26.186219Z",
     "start_time": "2023-11-13T18:28:58.630214Z"
    }
   },
   "id": "5e01ca613ae14735"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating Model Performance with ROUGE Scores\n",
    "\n",
    "In this cell, we assess the performance of our text summarization model using the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric:\n",
    "\n",
    "- **ROUGE Metric Initialization**:\n",
    "  - We load the ROUGE scoring function from the `evaluate` library. ROUGE is a standard metric for evaluating text summarization models, focusing on the overlap between the predicted and actual summaries.\n",
    "\n",
    "- **Computing ROUGE Scores**:\n",
    "  - The `rouge_score.compute` method is called with our model's predictions and the actual summaries. This computation provides scores for different ROUGE metrics (like ROUGE-1, ROUGE-2, and ROUGE-L), reflecting various aspects of similarity between the predicted and actual texts.\n",
    "\n",
    "- **Results Visualization and Analysis**:\n",
    "  - The computed ROUGE scores are converted into a DataFrame, `rouge_scores_df`, for better visualization and ease of analysis.\n",
    "  - The first few rows of this DataFrame are displayed to give an immediate sense of the model's summarization performance.\n",
    "\n",
    "Evaluating the model with ROUGE scores is crucial for understanding its effectiveness in generating coherent, relevant, and concise summaries, and for identifying areas for further improvement."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75f36c55d286e61e"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "                  0\nrouge1     0.775264\nrouge2     0.696991\nrougeL     0.611014\nrougeLsum  0.611888",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>rouge1</th>\n      <td>0.775264</td>\n    </tr>\n    <tr>\n      <th>rouge2</th>\n      <td>0.696991</td>\n    </tr>\n    <tr>\n      <th>rougeL</th>\n      <td>0.611014</td>\n    </tr>\n    <tr>\n      <th>rougeLsum</th>\n      <td>0.611888</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the ROUGE scoring function for evaluating text summarization\n",
    "rouge_score = evaluate.load(\"rouge\")\n",
    "\n",
    "# Compute the ROUGE scores comparing the model's predictions with the actual summaries\n",
    "scores = rouge_score.compute(\n",
    "    predictions=results['predictions'], \n",
    "    references=results['actuals']\n",
    ")\n",
    "\n",
    "# Convert the scores to a DataFrame for better visualization and analysis\n",
    "rouge_scores_df = pd.DataFrame([scores]).transpose()\n",
    "\n",
    "# Display the first few rows of the ROUGE scores\n",
    "rouge_scores_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T18:31:27.828696Z",
     "start_time": "2023-11-13T18:31:26.184962Z"
    }
   },
   "id": "dd1d173725601f07"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
